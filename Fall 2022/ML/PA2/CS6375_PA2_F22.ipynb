{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CS6375.004 - MACHINE LEARNING - PROGRAMMING ASSIGNMENT 2**\n",
        "\n",
        "SATWIK ARVAPALLI(**SXA220012**) & ABHINAV YALAMADDI(**YXA210040**)"
      ],
      "metadata": {
        "id": "p3-JLhJFYDBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CiKIlgC3xtdp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def partition(x):\n",
        "    \"\"\"\n",
        "    Partition the column vector x into subsets indexed by its unique values (v1, ... vk)\n",
        "\n",
        "    Returns a dictionary of the form\n",
        "    { v1: indices of x == v1,\n",
        "      v2: indices of x == v2,\n",
        "      ...\n",
        "      vk: indices of x == vk }, where [v1, ... vk] are all the unique values in the vector z.\n",
        "    \n",
        "   \"\"\"\n",
        "    partitionVector = {}\n",
        "    for i in x:\n",
        "        partitionVector[i] = []\n",
        "        \n",
        "    for i in range(len(x)):\n",
        "        k=partitionVector[x[i]]\n",
        "        k.append(i)\n",
        "        \n",
        "    return partitionVector\n",
        "\n",
        "    raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "yNqoEvf7x2LS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y, weight):\n",
        "    \"\"\"\n",
        "    Compute the entropy of a vector y by considering the counts of the unique values (v1, ... vk), in z\n",
        "\n",
        "    Returns the entropy of z: H(z) = p(z=v1) log2(p(z=v1)) + ... + p(z=vk) log2(p(z=vk))\n",
        "    \"\"\"\n",
        "    entropy = 0.0\n",
        "    p_of_y = 0.0\n",
        "\n",
        "    class_0 = np.sum(weight[y == 0])\n",
        "    class_1 = np.sum(weight[y == 1])\n",
        "    class_sum = class_0 + class_1\n",
        "    partition_of_y = partition(y)\n",
        "    \n",
        "    probability_of_class_0 = class_0 / class_sum\n",
        "    probability_of_class_1 = class_1 / class_sum\n",
        "    # if probability_of_class_0:\n",
        "    #   entropy += probability_of_class_0 * math.log2(probability_of_class_0) * -1\n",
        "    # if probability_of_class_1:\n",
        "    #   entropy += probability_of_class_1 * math.log2(probability_of_class_1) * -1    \n",
        "\n",
        "    for key in partition_of_y:\n",
        "        ind = partition_of_y[key]\n",
        "        n = len(partition_of_y[key])\n",
        "        sum_of_weights = 0\n",
        "        for k in partition_of_y[key]:\n",
        "          sum_of_weights = sum_of_weights + weight[k]\n",
        "        p_of_y = sum_of_weights/(np.sum(weight))\n",
        "        entropy +=  (-1*p_of_y*(math.log(p_of_y,2)))  \n",
        "    return entropy\n",
        "\n",
        "    raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "4IHHpQhfx2yd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mutual_information(x, y, weight):\n",
        "    \"\"\"\n",
        "    Compute the mutual information between a data column (x) and the labels (y). The data column is a single attribute\n",
        "    over all the examples (n x 1). Mutual information is the difference between the entropy BEFORE the split set, and\n",
        "    the weighted-average entropy of EACH possible split.\n",
        "\n",
        "    Returns the mutual information: I(x, y) = H(y) - H(y | x)\n",
        "    \"\"\"\n",
        "    h_of_y = entropy(y, weight)\n",
        "    partition_of_x = partition(x)\n",
        "    h_of_yx=0.00\n",
        "    \n",
        "    for key in partition_of_x:\n",
        "      wt = []\n",
        "      temp = []\n",
        "      for i in partition_of_x[key]:           \n",
        "          temp.append(y[i])\n",
        "      for i in partition_of_x[key]:           \n",
        "          wt.append(weight[i]) \n",
        "      h_of_key = entropy(temp, wt) \n",
        "      #p_of_key = len(partition_of_x[key])/len(x)\n",
        "      #(np.sum(weight[indices])/np.sum(weight))\n",
        "      # sum_of_weights = 0.00\n",
        "      # for k in partition_of_x[key]:\n",
        "      #   sum_of_weights = sum_of_weights + weight[k]\n",
        "      p_of_key = np.sum(wt)/(np.sum(weight))\n",
        "\n",
        "        \n",
        "      h_of_yx = h_of_yx + (p_of_key * h_of_key)\n",
        "     \n",
        "    i_of_xy = h_of_y - h_of_yx\n",
        "    \n",
        "    return i_of_xy\n",
        "\n",
        "    raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "J3vnv7c-x3GT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id3(x, y, weight, used_attribute_pairs=None, depth=0, max_depth=3):\n",
        "    \"\"\"\n",
        "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
        "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
        "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
        "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
        "           value of y (majority label)\n",
        "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
        "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
        "    and partitions the data set based on the values of that attribute before the next recursive call to ID3.\n",
        "\n",
        "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
        "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
        "    (taking values a, b, c) and x2 (taking values d, e), the initial attribute value pair list is a list of all pairs of\n",
        "    attributes with their corresponding values:\n",
        "    [(x1, a),\n",
        "     (x1, b),\n",
        "     (x1, c),\n",
        "     (x2, d),\n",
        "     (x2, e)]\n",
        "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
        "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
        "\n",
        "    The tree is stored as a nested dictionary, where each entry is of the form\n",
        "                    (attribute_index, attribute_value, True/False): subtree\n",
        "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
        "    indicates that we test if (x4 == 2) at the current node.\n",
        "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
        "    * Leaf nodes are (majority) class labels\n",
        "\n",
        "    Returns a decision tree represented as a nested dictionary, for example\n",
        "    {(4, 1, False):\n",
        "        {(0, 1, False):\n",
        "            {(1, 1, False): 1,\n",
        "             (1, 1, True): 0},\n",
        "         (0, 1, True):\n",
        "            {(1, 1, False): 0,\n",
        "             (1, 1, True): 1}},\n",
        "     (4, 1, True): 1}\n",
        "    \"\"\"\n",
        "    tree = {}\n",
        "    \n",
        "    if len(x)==0 or len(y)==0:\n",
        "        return\n",
        "\n",
        "    if(len(partition(y)) < 2):\n",
        "        return y[0]\n",
        "\n",
        "    if used_attribute_pairs != None and len(used_attribute_pairs) == 0:\n",
        "      true_count=0\n",
        "      false_count=0\n",
        "      for i in range(len(y)):\n",
        "          if y[i] == 1:\n",
        "            true_count=true_count+1\n",
        "          else:\n",
        "            false_count=false_count+1\n",
        "      if true_count > false_count:\n",
        "          return 1\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "    if depth==max_depth:\n",
        "        true_count=0\n",
        "        false_count=0\n",
        "        for i in range(len(y)): \n",
        "            if y[i] == 1: \n",
        "                true_count=true_count+1\n",
        "            else:\n",
        "                false_count=false_count+1\n",
        "        if true_count > false_count:\n",
        "            return 1\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "\n",
        "    mutual_info = {}\n",
        "    shape1=np.shape(x)\n",
        "    for j in range(0,shape1[1]):\n",
        "        xi=[]\n",
        "        for k in range(0,shape1[0]):\n",
        "            xi.append(x[k][j])\n",
        "        temp_part = partition(xi)\n",
        "        for key in temp_part:\n",
        "            partition_on_key = [0 for it in range(0, len(xi))]\n",
        "            for i in temp_part[key]:\n",
        "              partition_on_key[i] = 1\n",
        "                \n",
        "            temp = mutual_information(partition_on_key, y, weight)\n",
        "            mutual_info[(j,key)] = temp\n",
        "      \n",
        "    if used_attribute_pairs == None:\n",
        "      used_attribute_pairs = list(mutual_info.keys())\n",
        "    else:\n",
        "      for key in list(mutual_info.keys()):\n",
        "        if key not in used_attribute_pairs:\n",
        "          mutual_info.pop(key)\n",
        "\n",
        "    if len(mutual_info) == 0:\n",
        "      return\n",
        "\n",
        "    xi_to_partition = max(mutual_info, key = mutual_info.get)\n",
        "    \n",
        "    mutual_info_of_xy = max(mutual_info, default=0.0)\n",
        "\n",
        "    y_true=[]\n",
        "    y_false=[]\n",
        "    x_true=[]\n",
        "    x_false=[]\n",
        "    weight_of_true = []\n",
        "    weight_of_false = []\n",
        "    for i in range(len(y)):\n",
        "        if (x[i][xi_to_partition[0]] == xi_to_partition[1]):\n",
        "            y_true.append(y[i])\n",
        "            x_true.append(x[i])\n",
        "            weight_of_true.append(weight[i])\n",
        "        else:\n",
        "            y_false.append(y[i])\n",
        "            x_false.append(x[i])\n",
        "            weight_of_false.append(weight[i])\n",
        "    \n",
        "\n",
        "\n",
        "    updated_pairs=used_attribute_pairs[:]\n",
        "    updated_pairs.remove(xi_to_partition)\n",
        "\n",
        "    #if xi_to_partition in used_attribute_pairs: del used_attribute_pairs[xi_to_partition]\n",
        "\n",
        "    tree[(xi_to_partition[0],xi_to_partition[1],True)]=id3(x_true,y_true,weight_of_true,updated_pairs,depth+1,max_depth)\n",
        "    \n",
        "    tree[(xi_to_partition[0],xi_to_partition[1], False)]=id3(x_false,y_false,weight_of_false, updated_pairs,depth+1,max_depth)\n",
        "\n",
        "\n",
        "    return tree\n",
        "    \n",
        "   # raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "MzSS_-95AI6N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the average error between the true labels (y_true) and the predicted labels (y_pred)\n",
        "\n",
        "    Returns the error = (1/n) * sum(y_true != y_pred)\n",
        "    \"\"\"\n",
        "    count=0\n",
        "    n = len(y_true)\n",
        "    for i in range(n):\n",
        "        if y_true[i]!=y_pred[i]:\n",
        "            count = count + 1\n",
        "    return count/n    \n",
        "    \n",
        "    raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "Fh7ExD8vyPUa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bagging(x,y,max_depth,num_trees):\n",
        "\timport random\n",
        "\trandom.seed(0)\n",
        "\tlenX = len(x)\n",
        "\tsequence = list(range(len(x)))\n",
        "\tweight = np.ones(lenX)\n",
        "\thypothesis = {}\n",
        "\talpha = 1\n",
        "\n",
        "\tfor tn in range(num_trees):\n",
        "\t\tindices = random.choices(sequence,k=lenX)\n",
        "\t\t\n",
        "\t\tdecision_tree = id3(x[indices], y[indices],weight, max_depth=max_depth)\n",
        "\t\thypothesis[tn] = (alpha,decision_tree)\n",
        "\t\n",
        "\treturn hypothesis"
      ],
      "metadata": {
        "id": "vbli9lC2Sz6Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boosting(x,y,max_depth,num_stumps):\n",
        "\tlenX = len(x)\n",
        "\thypothesis = {}\n",
        "\tweight = np.ones(lenX)/lenX\n",
        "\n",
        "\tfor ns in range(num_stumps):\n",
        "\t\tdecision_tree = id3(x, y, weight, max_depth=max_depth)\n",
        "    \n",
        "\t\ty_pred = [predict_example_base_learner(xe, decision_tree) for xe in x]\n",
        "\n",
        "\t\t#y_pred1 = [predict_example(xe, decision_tree, \"boosting\") for xe in x]\n",
        "\n",
        "\t\tepsilon = (np.dot(np.absolute(y - y_pred), weight))/ np.sum(weight)\n",
        "\t\talpha = 0.5 * (np.log(((1 - epsilon) / epsilon)))\n",
        "\t\t# print(alpha)\n",
        "\t\t# print(epsilon)\n",
        "\t\tfor i in range(len(y_pred)):\n",
        "\t\t\tif y_pred[i] == y[i]:\n",
        "\t\t\t\tweight[i] *= np.exp(-alpha)\n",
        "\t\t\telse:\n",
        "\t\t\t\tweight[i] *= np.exp(alpha)\n",
        "\n",
        "\t\t#weight /= 2 * np.sqrt(epsilon * (1 - epsilon))\n",
        "\t\thypothesis[ns] = (alpha, decision_tree)\n",
        "\n",
        "\n",
        "\treturn hypothesis"
      ],
      "metadata": {
        "id": "JdYCYlpAS1Mw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_example(x,h_ens,ensemble_type):\n",
        "\t\"\"\"\n",
        "\th_ens is an ensemble of weighted hypotheses.\n",
        "\tThe ensemble is represented as an array of pairs [(alpha_i, h_i)], where each hypothesis and weight\n",
        "\tare represented by the pair: (alpha_i, h_i).\n",
        "\t\"\"\"\n",
        "\tif ensemble_type == \"bagging\":\n",
        "\t\tpredictions = []\n",
        "\t\tfor k in h_ens:\n",
        "\t\t\ty_pred = predict_example_base_learner(x,h_ens[k][1])\n",
        "\t\t\tpredictions.append(y_pred)\n",
        "\n",
        "\t\tpredict_egz = max(predictions, key=predictions.count)\n",
        "\t\treturn predict_egz\n",
        "\telse:\n",
        "\t\tpredictions = []\n",
        "\t\tsum_alpha = 0\n",
        "\n",
        "\t\tfor y in h_ens:\n",
        "\t\t\talpha, tree = h_ens[y]\n",
        "\t\t\ttst_pred = predict_example_base_learner(x, tree)\n",
        "\n",
        "\t\t\tpredictions.append(tst_pred*alpha)\n",
        "\t\t\tsum_alpha += alpha\n",
        "\t\tpredict_egz = np.sum(predictions) / sum_alpha\n",
        "\t\tif predict_egz >= 0.5:\n",
        "\t\t\treturn 1\n",
        "\t\telse:\n",
        " \t\t\treturn 0"
      ],
      "metadata": {
        "id": "WIg3k8BDTCp_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_example_base_learner(x, tree):\n",
        "    \"\"\"\n",
        "    Predicts the classification label for a single example x using tree by recursively descending the tree until\n",
        "    a label/leaf node is reached.\n",
        "\n",
        "    Returns the predicted label of x according to tree\n",
        "    \"\"\"\n",
        "    if type(tree) is not dict: \n",
        "      return tree\n",
        "        \n",
        "    if x[list(tree.keys())[0][0]]==list(tree.keys())[0][1]:\n",
        "        temp = True\n",
        "    else:\n",
        "        temp = False\n",
        "        \n",
        "    if type(tree[(list(tree.keys())[0][0],list(tree.keys())[0][1],temp)]) is dict:        \n",
        "        return predict_example_base_learner(x,tree[(list(tree.keys())[0][0],list(tree.keys())[0][1],temp)])\n",
        "    \n",
        "    return tree[(list(tree.keys())[0][0],list(tree.keys())[0][1],temp)]\n",
        "\n",
        "    raise Exception('Function not yet implemented!')"
      ],
      "metadata": {
        "id": "NHRRigqGx38T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(tree, depth=0):\n",
        "    \"\"\"\n",
        "    Pretty prints (kinda ugly, but hey, it's better than nothing) the decision tree to the console. Use print(tree) to\n",
        "    print the raw nested dictionary representation.\n",
        "    DO NOT MODIFY THIS FUNCTION!\n",
        "    \"\"\"\n",
        "\n",
        "    if depth == 0:\n",
        "        print('TREE')\n",
        "\n",
        "    for index, split_criterion in enumerate(tree):\n",
        "        sub_trees = tree[split_criterion]\n",
        "\n",
        "        # Print the current node: split criterion\n",
        "        print('|\\t' * depth, end='')\n",
        "        print('+-- [SPLIT: x{0} = {1} {2}]'.format(split_criterion[0], split_criterion[1], split_criterion[2]))\n",
        "\n",
        "        # Print the children\n",
        "        if type(sub_trees) is dict:\n",
        "            visualize(sub_trees, depth + 1)\n",
        "        else:\n",
        "            print('|\\t' * (depth + 1), end='')\n",
        "            print('+-- [LABEL = {0}]'.format(sub_trees))"
      ],
      "metadata": {
        "id": "qoR5azgzySBL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_mat(y_true,y_pred):\n",
        "\ttp,tn,fp,fn = 0,0,0,0\n",
        "\tfor i in range(len(y_pred)):\n",
        "\t\tif y_pred[i] == 1 and y_true[i] == 1:\n",
        "\t\t\ttp += 1\n",
        "\t\telif y_pred[i] == 0 and y_true[i] == 0:\n",
        "\t\t\ttn += 1\n",
        "\t\telif y_pred[i] == 1 and y_true[i] == 0:\n",
        "\t\t\tfp += 1\n",
        "\t\telif y_pred[i] == 0 and y_true[i] == 1:\n",
        "\t\t\tfn += 1\n",
        "\t# mat = np.array([tn,fp,fn,tp])\t\t#This is simlar to sklearn convention\n",
        "\tmat = np.array([tp,fn,fp,tn]).reshape(2,2)\t\t#This is for current assignment\n",
        "\tprint(\"\\t\\tClassifier Prediction\")\n",
        "\tprint(\"\\t\\t\\tPositive\\tNegative\")\n",
        "\tprint(\"Actual | Positive\\t\",mat[0][0],\"\\t\\t\",mat[0][1])\n",
        "\tprint(\"Value  | Negative\\t\",mat[1][0],\"\\t\\t\",mat[1][1])"
      ],
      "metadata": {
        "id": "Yuo2EcCR1wel"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the training data\n",
        "    M = np.genfromtxt('./mushroom.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
        "    ytrn = M[:, 0]\n",
        "    Xtrn = M[:, 1:]\n",
        "\n",
        "    # Load the test data\n",
        "    M = np.genfromtxt('./mushroom.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
        "    ytst = M[:, 0]\n",
        "    Xtst = M[:, 1:]\n",
        "\n",
        "    testing = []\n",
        "    # Bagging\n",
        "    print(\"---------------------BAGGING---------------------\")\n",
        "    for depth in [3,5]:\n",
        "      for bag_size in [10,20]:\n",
        "        # Learn a decision tree of depth dep\n",
        "        print(\"Depth: \", depth, \"Bag Size: \", bag_size)\n",
        "        ensemble_bag = bagging(Xtrn,ytrn,depth,bag_size)\n",
        "\n",
        "        # Compute the test error\n",
        "        y_pred = [predict_example(x, ensemble_bag, \"bagging\") for x in Xtst]\n",
        "        # print(y_pred)\n",
        "        \n",
        "        tst_err = compute_error(ytst, y_pred)\n",
        "        testing.append(tst_err*100)\n",
        "        \n",
        "        # # print('depth=',depth, end=\" \")\n",
        "        print('Test Error = {0:4.2f}%'.format(tst_err * 100))\n",
        "        confusion_mat(ytst,y_pred)\n",
        "        \n",
        "    \n",
        "    # Boosting\n",
        "    print(\"---------------------Boosting---------------------\")\n",
        "    for depth in [1,2]:\n",
        "      for bag_size in [20,40]:\n",
        "        # Learn a decision tree of depth dep\n",
        "        print(\"Depth: \", depth, \"bag_size: \", bag_size)\n",
        "        ensemble_boost = boosting(Xtrn,ytrn,depth,bag_size)\n",
        "\n",
        "        # # Compute the test error\n",
        "        y_pred = [predict_example(x, ensemble_boost, \"boosting\") for x in Xtst]\n",
        "        # # print(y_pred)\n",
        "        \n",
        "        tst_err = compute_error(ytst, y_pred)\n",
        "        testing.append(tst_err*100)\n",
        "        # print('depth=',depth, end=\" \")\n",
        "        print('Test Error = {0:4.2f}%'.format(tst_err * 100))\n",
        "        confusion_mat(ytst,y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oiypi4I1yVWU",
        "outputId": "2606346d-c866-437b-df38-deef791a372b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------BAGGING---------------------\n",
            "Depth:  3 Bag Size:  10\n",
            "Test Error = 4.23%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 815 \t\t 29\n",
            "Value  | Negative\t 57 \t\t 1130\n",
            "Depth:  3 Bag Size:  20\n",
            "Test Error = 4.23%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 815 \t\t 29\n",
            "Value  | Negative\t 57 \t\t 1130\n",
            "Depth:  5 Bag Size:  10\n",
            "Test Error = 0.20%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 844 \t\t 0\n",
            "Value  | Negative\t 4 \t\t 1183\n",
            "Depth:  5 Bag Size:  20\n",
            "Test Error = 0.20%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 844 \t\t 0\n",
            "Value  | Negative\t 4 \t\t 1183\n",
            "---------------------Boosting---------------------\n",
            "Depth:  1 bag_size:  20\n",
            "Test Error = 11.18%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 793 \t\t 51\n",
            "Value  | Negative\t 176 \t\t 1011\n",
            "Depth:  1 bag_size:  40\n",
            "Test Error = 11.18%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 793 \t\t 51\n",
            "Value  | Negative\t 176 \t\t 1011\n",
            "Depth:  2 bag_size:  20\n",
            "Test Error = 6.40%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 823 \t\t 21\n",
            "Value  | Negative\t 109 \t\t 1078\n",
            "Depth:  2 bag_size:  40\n",
            "Test Error = 6.40%\n",
            "\t\tClassifier Prediction\n",
            "\t\t\tPositive\tNegative\n",
            "Actual | Positive\t 823 \t\t 21\n",
            "Value  | Negative\t 109 \t\t 1078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_training_set = [\"train\"]\n",
        "sklearn_testing_set = [\"test\"]\n",
        "sklearn_names_set = [\"mushroom data\"]\n",
        "\n",
        "\n",
        "for train,test,name in zip(sklearn_training_set,sklearn_testing_set,sklearn_names_set):\n",
        "  # Load the training data\n",
        "    M = np.genfromtxt('./mushroom.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
        "    ytrn = M[:, 0]\n",
        "    Xtrn = M[:, 1:]\n",
        "\n",
        "    # Load the test data\n",
        "    M = np.genfromtxt('./mushroom.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
        "    ytst = M[:, 0]\n",
        "    Xtst = M[:, 1:]\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "\n",
        "max_depth = [3,5]\n",
        "bag_size = [10,20]\n",
        "\n",
        "for md in max_depth:\n",
        "  for bs in bag_size:\n",
        "    \n",
        "    print(\"Bagging : max_depth =\",md,\"bag_size = \",bs)\n",
        "    clf = BaggingClassifier(tree.DecisionTreeClassifier(random_state = 42, max_depth = md),n_estimators = bs)\n",
        "    clf = clf.fit(Xtrn, ytrn)\n",
        "    y_pred = clf.predict(Xtst)\n",
        "    accuracy = accuracy_score(ytst,y_pred)\n",
        "    print(\"Test Error = \", (1-accuracy)*100)\n",
        "    print(\"Confusion matrix: \\n\",confusion_matrix(ytst,y_pred))\n",
        "    # print(\"\\n\\n\")\n",
        "\n",
        "max_depth = [1,2]\n",
        "bag_size = [20,40]\n",
        "\n",
        "for md in max_depth:\n",
        "  for bs in bag_size:\n",
        "    \n",
        "    print(\"AdaBoost : max_depth =\",md,\"bag_size = \",bs)\n",
        "    clf = AdaBoostClassifier(tree.DecisionTreeClassifier(random_state = 42, max_depth = md),n_estimators = bs)\n",
        "    clf = clf.fit(Xtrn, ytrn)\n",
        "    y_pred = clf.predict(Xtst)\n",
        "    accuracy = accuracy_score(ytst,y_pred)\n",
        "    print(\"Test Error = \", (1-accuracy)*100)\n",
        "    print(\"Confusion matrix: \\n\",confusion_matrix(ytst,y_pred))"
      ],
      "metadata": {
        "id": "Gu8hyo7FjPFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fa02e6-10f9-405f-8628-98b7a5828f96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging : max_depth = 3 bag_size =  10\n",
            "Test Error =  4.382077794190053\n",
            "Confusion matrix: \n",
            " [[1102   85]\n",
            " [   4  840]]\n",
            "Bagging : max_depth = 3 bag_size =  20\n",
            "Test Error =  4.382077794190053\n",
            "Confusion matrix: \n",
            " [[1102   85]\n",
            " [   4  840]]\n",
            "Bagging : max_depth = 5 bag_size =  10\n",
            "Test Error =  1.1816838995568735\n",
            "Confusion matrix: \n",
            " [[1187    0]\n",
            " [  24  820]]\n",
            "Bagging : max_depth = 5 bag_size =  20\n",
            "Test Error =  1.1816838995568735\n",
            "Confusion matrix: \n",
            " [[1187    0]\n",
            " [  24  820]]\n",
            "AdaBoost : max_depth = 1 bag_size =  20\n",
            "Test Error =  0.1969473165928104\n",
            "Confusion matrix: \n",
            " [[1185    2]\n",
            " [   2  842]]\n",
            "AdaBoost : max_depth = 1 bag_size =  40\n",
            "Test Error =  0.0\n",
            "Confusion matrix: \n",
            " [[1187    0]\n",
            " [   0  844]]\n",
            "AdaBoost : max_depth = 2 bag_size =  20\n",
            "Test Error =  0.0\n",
            "Confusion matrix: \n",
            " [[1187    0]\n",
            " [   0  844]]\n",
            "AdaBoost : max_depth = 2 bag_size =  40\n",
            "Test Error =  0.0\n",
            "Confusion matrix: \n",
            " [[1187    0]\n",
            " [   0  844]]\n"
          ]
        }
      ]
    }
  ]
}